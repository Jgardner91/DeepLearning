{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "marine-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import shutil as sh\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alike-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = [['siberian'],['brittish','shorthair'],['persian'],\n",
    "              ['main','coon'],['ragdoll'],['american','shorthair'],['abyssinian'],['scottish','fold'],\n",
    "             ['norwegian','forest'],['burmese'],['birman'],['russian','blue'],['japanese','bobtail'],\n",
    "             ['munchkin'],['bombay'],['manx'],['oriental','shorthair'],['cornish','rex'],['turkish','angora'],\n",
    "             ['american','bobtail'],['himalayan'],['chartreux'],['tonkinese'],['turkish','van'],['LaPerm'],\n",
    "             ['snowshoe'],['peterbald'],['havana','brown'],['brumilla'],['lykoi'],['cymric'],['donskoy'],['chausie'],\n",
    "             ['ragamuffin'],['selkirk','rex'],['american','wirehair']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "asian-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_query(query_list):\n",
    "    chromedriver = \"/Applications/chromedriver\" \n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    extensions = []\n",
    "    # google images search query --- > 'cat type + cat(noun)' \n",
    "    for i in range(len(query_list)):\n",
    "        if len(query_list[i]) == 1:\n",
    "            url = 'https://www.google.com/search?q={}+cat&tbm=isch&ved'.format(query_list[i][0])\n",
    "        if len(query_list[i]) == 2:\n",
    "            url = 'https://www.google.com/search?q={}+{}+cat&tbm=isch&ved'.format(query_list[i][0],query_list[i][1])\n",
    "        response = requests.get(url)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        driver = webdriver.Chrome(chromedriver)\n",
    "        driver.get(url)\n",
    "        driver.page_source[:]\n",
    "        time.sleep(3)\n",
    "\n",
    "        # defining soup object \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        suffix = [item['data-id'] for item in soup.find_all('div', attrs={'data-id' : True})]\n",
    "        extensions.append(suffix)\n",
    "        driver.close()\n",
    "    return extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "frank-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting image extension lists\n",
    "#extensions = google_query(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "known-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing images for each bread to file 'Cats'\n",
    "def write_images_to_file(extensions):\n",
    "    for j in range(11,len(extensions)):\n",
    "        for i in range(len(extensions[j])):\n",
    "\n",
    "            chromedriver = \"/Applications/chromedriver\" \n",
    "            os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "\n",
    "            if len(query_list[j]) == 1:\n",
    "                # google images search query --- > 'breadname + cat'\n",
    "                url_2 = 'https://www.google.com/search?q={}+cat&tbm=isch&ved#imgrc={}'.format(query_list[j][0],extensions[j][i])\n",
    "            if len(query_list[j]) == 2:\n",
    "                # google images search query --- > 'breadname + breadname + cat'\n",
    "                url_2 = 'https://www.google.com/search?q={}+{}+cat&tbm=isch&ved#imgrc={}'.format(query_list[j][0],query_list[j][1],extensions[j][i])\n",
    "        \n",
    "            page = response.text\n",
    "            soup_2 = BeautifulSoup(page, \"lxml\")\n",
    "            driver = webdriver.Chrome(chromedriver)\n",
    "            driver.get(url_2)\n",
    "            driver.page_source[:]\n",
    "            time.sleep(5)\n",
    "\n",
    "            # defining soup object \n",
    "            soup_2 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # finding class that each image is in \n",
    "            find_class = soup_2.find_all(class_ = 'v4dQwb')\n",
    "            \n",
    "            # making sure class isnt empty \n",
    "            if find_class != []:\n",
    "                for k in range(len(find_class)):\n",
    "                    # finding where in the class list , the jpg link is \n",
    "                    if 'jpg' in str(find_class[k]):\n",
    "                        iD = k\n",
    "                        \n",
    "                        # getting 'a' tag list  \n",
    "                        a_tag = find_class[iD].find_all('a')\n",
    "                        # getting img tag\n",
    "                        image = a_tag[0].find_all('img')\n",
    "                        # using 'src' attribute to find link \n",
    "                        link = image[0].attrs['src']\n",
    "                     \n",
    "                        # trying to avoid request erros \n",
    "                        if 'https' in link and link[-1] == 'g':\n",
    "                            full_url = link\n",
    "        \n",
    " \n",
    "                            if '.au' not in link:\n",
    "                                r = requests.get(full_url, stream=True) #Get request on full_ur\n",
    "                                if r.status_code == 200:\n",
    "                                    # writing image to file \n",
    "                                    with open(\"Cats/{}{}.jpg\".format(query_list[j][0],i), 'wb') as f: \n",
    "                                        r.raw.decode_content = True\n",
    "                                        sh.copyfileobj(r.raw, f)\n",
    "            driver.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-associate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
